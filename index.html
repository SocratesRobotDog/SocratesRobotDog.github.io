<!DOCTYPE html>


<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Towards Text-based Human Search and Approach using a Robot Dog">
  <meta property="og:title" content="SOCRATES"/>
  <meta property="og:description" content="Text-based Human Search and Approach using a Robot Dog"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->

  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="SOCRATES">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>SOCRATES: Text-based Human Search and Approach using a Robot Dog</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
        <div class="columns is-centered">
            <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">SOCRATES: Text-based Human Search and Approach using a Robot Dog</h1>
                <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://sites.google.com/view/cv-jeongeunpark-korea" target="_blank">Jeongeun Park</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/silveira-jefferson" target="_blank">Jefferson Silveria</a><sup>2</sup>,</span>
                    <span class="author-block">
                      <a href="https://www.matthewpan.com/" target="_blank">Matthew Pan</a><sup>2</sup>,</span>
                    <span class="author-block">
                            <a href="https://sites.google.com/view/sungjoon-choi/home" target="_blank">
                                Sungjoon Choi</a><sup>1<sup></span>
                    </div>
                <div class="is-size-5 publication-authors">
                    <span class="author-block">Korea University, Korea<sup>1</sup><br>
                        Queens University, Canada<sup>2</sup>
                    </span>
                    </div>
                <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2302.05324" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
            </div>
        </div>
        <!-- <h2 class="title is-3"> News</h2> -->
        <h3 class="title is-5"> Our paper got accepted at International Conference on Robot and Human Interactive Communication (RO-MAN), 2024. </h3>
    </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
                In this paper, we propose a SOCratic model for Robots Approaching humans based on TExt System (SOCRATES) focusing on the human search and approach based on free-form textual description;
                the robot first searches for the target user, then the robot proceeds to approach in a human-friendly manner. In particular, textual descriptions are composed of appearance 
                (e.g., "wearing white shirts with black hair") and location clues (e.g., "is a student who works with robots"). 
                We initially present a Human Search Socratic Model that connects large pre-trained models in the language domain to solve the downstream task, 
                which is searching for the target person based on textual descriptions. Then, we propose a hybrid learning-based framework for generating target-cordial robotic motion to approach a person, 
                consisting of a learning-from-demonstration module and a knowledge distillation module. We validate the proposed searching module via simulation using a virtual mobile robot as well as through 
                real-world experiments involving participants and the Boston Dynamics Spot robot. Furthermore, we analyze the properties of the proposed approaching framework with human participants based on 
                the Robotic Social Attributes Scale (RoSAS).
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- Teaser video-->
<section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="assets/intro.mp4"
          type="video/mp4">
        </video>
      </div>
    </div>
  </section>

  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <!-- Paper video. -->
        <h2 class="title is-3">Proposed method</h2>
        <div class="columns is-centered has-text-centered">
            <div class="publication-video">
                <video poster="" id="video2" autoplay controls muted loop height="80%">
                    <!-- Your video file here -->
                    <source src="assets/starter.mp4" type="video/mp4">
                  </video>
                  <h2>
                    The problem is separated into search and approach phases.
                  </h2>
            </div>
        </div>
        <h4 class="title is-4">Assumption and Input of the System</h4>
            <figure class="figure text-center">
                <img src="./assets/input.jpg" style="width: 60%">
        </figure>
        <h4 class="title is-4">Human Search Socratic Model</h4>
            <figure class="figure text-center">
                <img src="./assets/search.jpg" style="width: 80%">
            </figure>
        <h2>
            The Human Search Socratic Model is composed of three key components: a large-language model, a vision-language model, and a waypoint generator. 
            The large-language model takes the description of a target person and estimates the search prior, 
            whereas the vision-language model processes images and descriptions to localize the target person. 
            Lastly, the waypoint generator commands the robotic actions with the search prior and the detection results.
        </h2>
        <h4 class="title is-4">Hybrid Learning Based Human Approach</h4>
            <figure class="figure text-center">
                <img src="./assets/approach.jpg" style="width: 80%">
            </figure>
        <h2>
            We propose a hybrid learning-based framework for generating a cordial approach motion to a target person. 
            Target-friendly approach motions are an essential part of the "human search and approach" task since the robot has to move to a reachable position within the range of the 
            target person without appearing threatening or distracting him/her. The proposed hybrid learning-based framework consists of two different modules: 
            learning from demonstration (LfD) and knowledge distillation. Both modules estimate the reward function of the state space for a confiding approach trajectory and are 
            combined to a final reward function. Next, the path planner conducts cost-aware planning based on the estimated reward function and sends actions (velocity commands) to the robot.
        </h2>
        </div>
    </section>

    <!-- Paper video. -->
    <section class="hero is-small is-light">
        <div class="hero-body">
        <div class="container">
            <!-- Paper video. -->
            <h2 class="title is-3">Demonstration</h2>
            <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                
                <div class="publication-video">
                    <video poster="" id="video2" autoplay controls muted loop height="100%">
                        <!-- Your video file here -->
                        <source src="assets/method.mp4"
                        type="video/mp4">
                    </video>
                </div>
            </div>
            </div>
        </div>
        </div>
    </section>  

    <section class="hero is-small is-light">
        <div class="hero-body">
          <div class="container">
            <!-- Paper video. -->
            <h2 class="title is-3">Experiment</h2>
            <h4 class="title is-4">Environment Setting</h4>
                <figure class="figure text-center">
                    <img src="./assets/env.jpg" style="width: 60%">
                </figure>
                <h2>
                    We use a Spot robot mounted with a Luxonis OAK-D-Pro camera. 
                    The real-world environment setting is a robotics lab environment, five different annotations of the floorplan are used to distinguish locations in the lab.
                </h2>

            <h4 class="title is-4">Search Demonstrations</h4>
            <div class="columns is-centered has-text-centered">
                <div class="publication-video">
                    <video poster="" id="video2" autoplay controls muted loop height="80%">
                        <!-- Your video file here -->
                        <source src="assets/demo1.mp4" type="video/mp4">
                </div>
            </div>
            <h4 class="title is-4">Approach Demonstrations</h4>
            <div class="columns is-centered has-text-centered">
                <div class="publication-video">
                    <video poster="" id="video2" autoplay controls muted loop height="80%">
                        <!-- Your video file here -->
                        <source src="assets/demo2.mp4" type="video/mp4">
                </div>
            </div>
            <h4 class="title is-4">Approach Comparison</h4>
            <div class="columns is-centered has-text-centered">
                <div class="publication-video">
                    <video poster="" id="video2" autoplay controls muted loop height="80%">
                        <!-- Your video file here -->
                        <source src="assets/demo3.mp4" type="video/mp4">
                </div>
            </div>
        </section>

    <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{park2023textbased,
        title={Towards Text-based Human Search and Approach with an Intelligent Robot Dog}, 
        author={Jeongeun Park and Jefferson Silveria and Matthew Pan and Sungjoon Choi},
        year={2024},
        booktitle={Proc. of the International Conference on Robot and Human Interactive Communication (RO-MAN)},
        organization={IEEE}
  }</code></pre>
    </div>
</section>

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!--End BibTex citation -->
